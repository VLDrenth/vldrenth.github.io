<html>
<head>
    <title>A different mixture of lasso and ridge</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Vincent Drenth - Personal Website'>
    <meta name='keywords' content='shrinkage, lasso, ridge'>
    <meta name='author' content='Vincent Drenth'>

    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
    displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
    <div class='front-matter'>
        <div class='wrap'>
            <h1>A different mixture of lasso and ridge</h1>
            <h4>An alternative to the elastic net</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published</h3>
                    <p>17 May 2025</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>If you know some high-dimensional statistics or machine learning, your toolkit probably has two heavy hitters for regularization: the Lasso ($l_1$ penalty) and Ridge regression ($l_2$ penalty).</p>

<p>We are generally taught a simple heuristic: use Lasso if you believe the truth is <strong>sparse</strong> (a few big signals, mostly zeros), and use Ridge if you believe the signal is <strong>dense</strong> (many small, non-zero effects). If you aren’t sure, you use the <strong>Elastic Net</strong>, which combines both penalties to try to get the best of both worlds.</p>

<p>I recently came across this interesting 2015 paper by Chernozhukov, Hansen, and Liao who say that there is a fundamental flaw in how we view this dichotomy. They introduce a method called <strong>Lava</strong>, based on a “Sparse + Dense” hypothesis. In the world of macro forecasting, this seems much closer to the ground truth, so I wanted to explore this a bit.</p>

<h3 id="the-sparse--dense-hypothesis">The “Sparse + Dense” Hypothesis</h3>

<p>The standard Elastic Net applies both penalties to the <em>same</em> parameter vector $\theta$. This forces the estimator to compromise. It tries to be sparse and dense at the same time, but because of the $l_1$ component, it will still force many coefficients to exactly zero .</p>

<p>However, real-world signals often look like a superposition of two distinct structures:</p>
<ol>
  <li>A <strong>dense</strong> part: Many variables having small, non-zero effects (like background noise or genetic traits with tiny individual impacts).</li>
  <li>A <strong>sparse</strong> part: A few variables having very large effects .</li>
</ol>

<p>The core idea of the Lava estimator is to decompose the parameter of interest, $\theta$, into two separate vectors:
\(\theta = \beta + \delta\)
Here, $\beta$ represents the dense signal, and $\delta$ represents the sparse signal .</p>

<h3 id="the-lava-estimator">The Lava Estimator</h3>

<p>Instead of penalizing $\theta$, Lava solves an optimization problem that penalizes $\beta$ and $\delta$ separately, using the penalties best suited for each.</p>

\[(\hat{\beta}, \hat{\delta}) = \arg \min_{\beta, \delta} \left\{ l(data, \beta + \delta) + \lambda_2 \|\beta\|_2^2 + \lambda_1 \|\delta\|_1 \right\}\]

<p>The final estimator is the sum: $\hat{\theta}_{lava} = \hat{\beta} + \hat{\delta}$ .</p>

<p>This distinction is subtle but powerful. Unlike the Elastic Net, which shrinks a single vector, Lava allows the “dense” part $\beta$ to pick up the small signals using Ridge regularization, while the “sparse” part $\delta$ uses Lasso regularization to capture the large spikes.</p>

<h3 id="visualizing-the-shrinkage">Visualizing the Shrinkage</h3>

<p>The intuition becomes much clearer when you look at the shrinkage functions: the map that transforms a raw observation $z$ into an estimated coefficient.</p>

<p>In the standard Lasso (or soft-thresholding), small coefficients are set strictly to zero. In Ridge, coefficients are scaled down linearly but never reach zero.</p>

<p>Lava does something unique. It acts as a bridge between the two:</p>
<ul>
  <li><strong>For small signals:</strong> It behaves like Ridge regression. The $l_2$ penalty dominates, preserving the small dense signals rather than zeroing them out.</li>
  <li><strong>For large signals:</strong> It behaves like the Lasso. The $l_1$ penalty takes over to handle the large outliers.</li>
  <li><strong>Continuity:</strong> It connects these two behaviors continuously .</li>
</ul>

<p>Because of the underlying $l_2$ penalty on the dense part, Lava never produces an exactly sparse solution (coefficients set to exactly zero) unless the input is exactly zero . This makes it distinct from the Elastic Net, which is essentially a sparsity-based method with some added shrinkage.</p>

<h3 id="why-lava">Why “Lava”?</h3>

<p>The authors named it “Lava” to emphasize that the method aims to capture or “wipe out” both sparse and dense signals simultaneously. Also, it just sounds a bit more interesting than lasso or ridge, so why not lava.</p>

<h3 id="does-it-work">Does it work?</h3>

<p>In simulations where the true signal is a mix of sparse and dense components, Lava strictly dominates both Lasso and Ridge.</p>
<ul>
  <li>If the signal is purely sparse, Lava performs as well as Lasso.</li>
  <li>If the signal is purely dense, Lava performs as well as Ridge.</li>
  <li>In the “middle ground” (Sparse + Dense), it significantly outperforms both.</li>
</ul>

<p>There is also a <strong>Post-Lava</strong> variant. Similar to the “Post-Lasso” technique, this involves running Lava to identify the structure, and then refitting the sparse part to remove the shrinkage bias introduced by the $l_1$ penalty.</p>

<h3 id="implementing">Implementing</h3>
<p>I have implemented the Lava Regression algorithm on <a href="https://github.com/vldrenth/lavapy">GitHub at vldrenth/lavapy</a>. To see just how simple it is, I started by implementing it with <code class="language-plaintext highlighter-rouge">cvxpy</code>. However, this uses general convex solvers, and we can do a lot better! It turns out, lava can be solved by just using lasso in a clever way.</p>

<h3 id="summary">Summary</h3>

<p>If you suspect your data contains a few needles in a haystack (sparse) <em>plus</em> a lot of hay that actually matters (dense), the Elastic Net might be over-sparsifying your model. Lava offers a mathematically sound way to model the superposition of these two signal types directly.</p>

    </div>
</div>
</body>
</html>