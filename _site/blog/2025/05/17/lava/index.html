<html>
<head>
    <title>A different mixture of lasso and ridge</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Vincent Drenth - Personal Website'>
    <meta name='keywords' content='shrinkage, lasso, ridge, lava, regularization'>
    <meta name='author' content='Vincent Drenth'>

    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
    displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
    <div class='front-matter'>
        <div class='wrap'>
            <h1>A different mixture of lasso and ridge</h1>
            <h4>An alternative to the elastic net</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published</h3>
                    <p>17 May 2025</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>When dealing with high-dimensional data, you’ll probably end up using one of two major methdos for regularization: the <strong>Lasso</strong> ($l_1$ penalty) and <strong>Ridge regression</strong> ($l_2$ penalty).</p>

<p>The way to choose between the two is usually as follows: use Lasso if you believe the truth is <strong>sparse</strong> (a few big signals, mostly zeros), and use Ridge if you believe the signal is <strong>dense</strong> (many small, non-zero effects). If you aren’t sure, or if you have many correlated variables that you suspect have non-zero effect, you use the <strong>Elastic Net</strong>, which combines both penalties to try to get the best of both worlds.</p>

<p>However, the 2015 paper by Chernozhukov, Hansen, and Liao, <em>A Lava Attack on the Recovery of Sums of Dense and Sparse Signals</em>, challenges the premise that the Elastic Net is the optimal compromise. They argue that applying both penalties to a single parameter vector introduces a fundamental conflict that compromises recovery quality. Instead, they propose a new methodology, the <strong>Lava</strong> estimator, which handles a signal structure that is often more representative of real-world data: the <strong>Sparse + Dense</strong> hypothesis.</p>

<hr />

<h2 id="the-fundamental-flaw-of-the-single-vector-penalty">The Fundamental Flaw of the Single-Vector Penalty</h2>

<p>The standard regularization approach assumes a homogeneous signal structure, meaning the underlying parameter vector $\theta$ is either purely sparse or purely dense, or at best, a single vector that is <em>both</em> penalized for sparsity and constrained in magnitude (Elastic Net).</p>

<p>The <strong>Elastic Net</strong> minimizes:
\(\min_{\theta} \left\{ l(\text{data}, \theta) + \lambda_2 \|\theta\|_2^2 + \lambda_1 \|\theta\|_1 \right\}\)
By combining the $l_1$ and $l_2$ penalties on the same vector $\theta$, the Elastic Net forces every single coefficient to compromise. The $l_1$ penalty strives to push small coefficients to <em>exactly zero</em> (selection), while the $l_2$ penalty strives to <em>shrink</em> all coefficients uniformly (stabilization).</p>

<p>If the true signal $\theta$ is a mix of many small effects (the “dense” part) and a few large effects (the “sparse” part), the Elastic Net makes a poor trade-off:</p>
<ol>
  <li>It severely <strong>shrinks</strong> the large, sparse coefficients due to the $l_2$ term, introducing significant bias.</li>
  <li>It aggressively <strong>zeros out</strong> the small, dense coefficients due to the $l_1$ term, losing valuable predictive information.</li>
</ol>

<p>The Elastic Net is, at its heart, still a <strong>sparsity-enforcing method</strong>; it is guaranteed to produce an estimate $\hat{\theta}$ with many exact zeros, when the penalty is sufficiently high.</p>

<hr />

<h2 id="the-sparse--dense-hypothesis-and-decomposition">The “Sparse + Dense” Hypothesis and Decomposition</h2>

<p>The core theoretical innovation of Lava is recognizing that the true signal $\theta$ is a <strong>combination</strong> of two distinct, non-homogeneous structures:</p>

\[\theta = \beta + \delta\]

<p>Where:</p>
<ul>
  <li>$\beta \in \mathbb{R}^p$ represents the <strong>dense signal</strong>. This vector is characterized by having many small, non-zero entries. This is the background structure, the cumulative effect of many variables.</li>
  <li>$\delta \in \mathbb{R}^p$ represents the <strong>sparse signal</strong>. This vector is characterized by having a few large, non-zero entries and the rest are exactly zero. This captures the few dominant drivers.</li>
</ul>

<p>This structure is highly relevant in fields like macroeconomics (where many indirect factors have small effects, but a few obviously important predictors have large effects).</p>

<hr />

<h2 id="the-lava-estimator-separate-penalization">The Lava Estimator: Separate Penalization</h2>

<p>Instead of penalizing the combined vector $\theta$, the Lava estimator lifts the problem to a $2p$-dimensional space and penalizes $\beta$ and $\delta$ separately, using the penalty best suited for each structure.</p>

<p>The optimization problem for Lava is defined as:</p>

\[(\hat{\beta}, \hat{\delta}) = \arg \min_{\beta, \delta} \left\{ l(\text{data}, \beta + \delta) + \lambda_2 \|\beta\|_2^2 + \lambda_1 \|\delta\|_1 \right\}\]

<p>The final estimator is the sum: $\hat{\theta}_{lava} = \hat{\beta} + \hat{\delta}$.</p>

<p><strong>Intuition Behind the Penalties:</strong></p>

<ol>
  <li><strong>$\lambda_2 |\beta|_2^2$ (Ridge on Dense):</strong> The Ridge penalty ($l_2$) is applied to $\beta$. This ensures that the dense component is <em>well-behaved</em> (coefficients are small and controlled) but <strong>non-sparse</strong>. Ridge never forces coefficients to zero unless the estimate is identically zero. This preserves the information contained in the small background signals.</li>
  <li><strong>$\lambda_1 |\delta|_1$ (Lasso on Sparse):</strong> The Lasso penalty ($l_1$) is applied to $\delta$. This ensures that the sparse component is, in fact, <strong>sparse</strong>, effectively identifying and separating the large spikes from the background noise.</li>
</ol>

<p>This formulation allows the estimator to effectively <em>decouple</em> the tasks of shrinkage and selection. The $l_2$ term handles magnitude control (shrinkage), and the $l_1$ term handles variable selection (sparsity).</p>

<p>Crucially, because of the $l_2$ penalty on the $\beta$ component, the final estimate $\hat{\theta}_{lava}$ is <strong>never exactly sparse</strong> (unless the truth is exactly zero). This distinguishes it fundamentally from Elastic Net and Lasso, which are designed to produce selection results.</p>

<hr />

<h2 id="visualizing-the-shrinkage-function">Visualizing the Shrinkage Function</h2>

<p>The most compelling insight into the Lava estimator’s behavior comes from its one-dimensional <strong>shrinkage function</strong>, $d(z)$. This function maps a raw observation $z$ (or single coefficient) to its estimated value $\hat{\theta}$.</p>

<p>Consider the one-dimensional case of estimating $\theta$ from a noisy observation $z = \theta + \epsilon$. The Lava estimator $\hat{\theta}_{lava}$ solves the simplified problem. The resulting closed-form solution for the shrinkage function, derived in the paper, reveals a piece-wise structure:</p>

\[d_{lava}(z) = \begin{cases}
z - \lambda_1 / 2, &amp; \text{if } z &gt; \tau \\
(1-k)z, &amp; \text{if } |z| \le \tau \\
z + \lambda_1 / 2, &amp; \text{if } z &lt; -\tau
\end{cases}\]

<p>Where $\tau = \lambda_1 / (2k)$ is the threshold, and $k = \lambda_2 / (1+\lambda_2)$ is a constant related to the Ridge parameter.</p>

<p>This function acts as a bridge between Lasso and Ridge, exhibiting optimal behavior in different regimes:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**For $</td>
          <td>z</td>
          <td>\le \tau$ (Small Signals):** The shrinkage is $d_{lava}(z) = (1-k)z$. This is the exact form of <strong>Ridge regression</strong> shrinkage. The small coefficients are scaled down but <strong>never set to zero</strong>. This preserves the dense signals.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**For $</td>
          <td>z</td>
          <td>&gt; \tau$ (Large Signals):** The shrinkage is $d_{lava}(z) = z \mp \lambda_1/2$. This is the exact form of <strong>Lasso (Soft-Thresholding)</strong> shrinkage, applied to the component that has passed the initial Ridge-like filter. This allows the large spikes to be estimated with less bias than a full $l_2$ penalty would impose.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Lava selectively applies the most beneficial shrinkage for the magnitude of the signal: Ridge for the small coefficients, and Lasso for the large ones, all while maintaining continuity.</p>

<hr />

<h2 id="computational-efficiency-solving-lava-via-lasso">Computational Efficiency: Solving Lava via Lasso</h2>

<p>Despite being defined over $2p$ variables, the Lava problem is convex and can be solved extremely efficiently. The authors’ <strong>Key Characterization</strong> (Theorem 3.1) proves that the Lava estimator can be computed by solving a standard Lasso problem on <em>transformed</em> data.</p>

<p>This is critical because it means existing, highly optimized Lasso solvers (like those based on coordinate descent) can be repurposed, avoiding the need for custom, slower general convex optimization routines.</p>

<h3 id="the-profiling-technique">The Profiling Technique</h3>

<ol>
  <li>
    <p><strong>Profile out $\beta$:</strong> For any fixed $\delta$, the optimal $\hat{\beta}(\delta)$ is the standard Ridge solution applied to the residuals $Y - X\delta$.
\(\hat{\beta}(\delta) = (X'X + n\lambda_2 I)^{-1} X'(Y - X\delta)\)</p>
  </li>
  <li>
    <p><strong>Transformed Lasso:</strong> By substituting this $\hat{\beta}(\delta)$ back into the objective function, the optimization collapses into a standard Lasso problem for $\delta$:
\(\hat{\delta} = \arg \min_{\delta} \frac{1}{n} \| \tilde{Y} - \tilde{X}\delta \|_2^2 + \lambda_1 \|\delta\|_1\)
Where $\tilde{X}$ and $\tilde{Y}$ are transformed versions of $X$ and $Y$ that incorporate the initial Ridge penalty $\lambda_2$.</p>
  </li>
</ol>

<p>The transformation effectively pre-whitens the data, accounting for the dense signal $\beta$ which is already being handled by the $l_2$ penalty. Once the data is transformed, the remaining signal is more purely sparse, allowing the Lasso to optimally isolate the $\delta$ spikes.</p>

<hr />

<h2 id="post-lava-for-bias-reduction">Post-Lava for Bias Reduction</h2>

<p>Like all $l_1$-penalized methods, the Lava estimator suffers from <strong>shrinkage bias</strong>. The $\lambda_1 |\delta|_1$ penalty pulls the estimated coefficients of the large spikes ($\hat{\delta}$) towards zero, even if they are important.</p>

<p>To mitigate this, the paper proposes a two-step procedure: <strong>Post-Lava</strong>.</p>

<ol>
  <li><strong>Run Lava:</strong> Compute $\hat{\theta}_{lava} = \hat{\beta} + \hat{\delta}$. This step is used <em>only</em> to identify the <strong>support set</strong> $\mathcal{S}$ (the indices where $\hat{\delta}_j \neq 0$).</li>
  <li><strong>Refit without penalty:</strong> Define $\hat{\theta}_{Post-Lava}$ as the estimate obtained by running a simple least squares regression using only the variables in the support set $\mathcal{S}$, but including the dense component $\beta$.</li>
</ol>

<p>This two-step process preserves the selection benefit of the $l_1$ penalty (identifying the correct sparse variables) but removes the bias introduced by the shrinkage, leading to estimates that are often more accurate in terms of prediction risk and parameter recovery.</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>The Lava estimator provides an elegant and efficient solution to a common high-dimensional modeling problem: the combination of sparse and dense signals. More importantly, it has a pretty cool name, which may or may not be why I <strong>really</strong> wanted to try it.</p>

<p>If you suspect your data contains a few needles in a haystack (sparse) <em>plus</em> a lot of hay that actually matters (dense) (Useful hay? Okay, my best analogy I admit…), the Elastic Net might be aggressively over-sparsifying your model and failing to recover the full signal. Lava’s decomposition and component-specific penalization strategy offer a (in theory at least) more sound approach for recovering the parameters. I still need to run a proper comparison on some actual data, because of course there is a reason Lasso and ridge have been popular for so long. That will be for another day, but I did implement it in Python on Github <a href="https://github.com/vldrenth/lavapy">vldrenth/lavapy</a>.</p>

    </div>
</div>
</body>
</html>