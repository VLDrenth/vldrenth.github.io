---
title: A different mixture of lasso and ridge
subtitle: An alternative to the elastic net
layout: default
date: 2025-05-17
keywords: shrinkage, lasso, ridge
categories: [statistics]
tags: [introduction, jekyll, blog]
published: true
---

If you know some high-dimensional statistics or machine learning, your toolkit probably has two heavy hitters for regularization: the Lasso ($l_1$ penalty) and Ridge regression ($l_2$ penalty). 

We are generally taught a simple heuristic: use Lasso if you believe the truth is **sparse** (a few big signals, mostly zeros), and use Ridge if you believe the signal is **dense** (many small, non-zero effects). If you aren't sure, you use the **Elastic Net**, which combines both penalties to try to get the best of both worlds.

I recently came across this interesting 2015 paper by Chernozhukov, Hansen, and Liao who say that there is a fundamental flaw in how we view this dichotomy. They introduce a method called **Lava**, based on a "Sparse + Dense" hypothesis. In the world of macro forecasting, this seems much closer to the ground truth, so I wanted to explore this a bit.

### The "Sparse + Dense" Hypothesis

The standard Elastic Net applies both penalties to the *same* parameter vector $\theta$. This forces the estimator to compromise. It tries to be sparse and dense at the same time, but because of the $l_1$ component, it will still force many coefficients to exactly zero .

However, real-world signals often look like a superposition of two distinct structures:
1.  A **dense** part: Many variables having small, non-zero effects (like background noise or genetic traits with tiny individual impacts).
2.  A **sparse** part: A few variables having very large effects .

The core idea of the Lava estimator is to decompose the parameter of interest, $\theta$, into two separate vectors:
$$\theta = \beta + \delta$$
Here, $\beta$ represents the dense signal, and $\delta$ represents the sparse signal .

### The Lava Estimator

Instead of penalizing $\theta$, Lava solves an optimization problem that penalizes $\beta$ and $\delta$ separately, using the penalties best suited for each.

$$(\hat{\beta}, \hat{\delta}) = \arg \min_{\beta, \delta} \left\{ l(data, \beta + \delta) + \lambda_2 \|\beta\|_2^2 + \lambda_1 \|\delta\|_1 \right\}$$

The final estimator is the sum: $\hat{\theta}_{lava} = \hat{\beta} + \hat{\delta}$ .

This distinction is subtle but powerful. Unlike the Elastic Net, which shrinks a single vector, Lava allows the "dense" part $\beta$ to pick up the small signals using Ridge regularization, while the "sparse" part $\delta$ uses Lasso regularization to capture the large spikes. 

### Visualizing the Shrinkage

The intuition becomes much clearer when you look at the shrinkage functions: the map that transforms a raw observation $z$ into an estimated coefficient.


In the standard Lasso (or soft-thresholding), small coefficients are set strictly to zero. In Ridge, coefficients are scaled down linearly but never reach zero.

Lava does something unique. It acts as a bridge between the two:
* **For small signals:** It behaves like Ridge regression. The $l_2$ penalty dominates, preserving the small dense signals rather than zeroing them out.
* **For large signals:** It behaves like the Lasso. The $l_1$ penalty takes over to handle the large outliers.
* **Continuity:** It connects these two behaviors continuously .

Because of the underlying $l_2$ penalty on the dense part, Lava never produces an exactly sparse solution (coefficients set to exactly zero) unless the input is exactly zero . This makes it distinct from the Elastic Net, which is essentially a sparsity-based method with some added shrinkage.

### Why "Lava"?

The authors named it "Lava" to emphasize that the method aims to capture or "wipe out" both sparse and dense signals simultaneously. Also, it just sounds a bit more interesting than lasso or ridge, so why not lava.

### Does it work?

In simulations where the true signal is a mix of sparse and dense components, Lava strictly dominates both Lasso and Ridge. 
* If the signal is purely sparse, Lava performs as well as Lasso.
* If the signal is purely dense, Lava performs as well as Ridge.
* In the "middle ground" (Sparse + Dense), it significantly outperforms both.

There is also a **Post-Lava** variant. Similar to the "Post-Lasso" technique, this involves running Lava to identify the structure, and then refitting the sparse part to remove the shrinkage bias introduced by the $l_1$ penalty.

### Implementing
I have implemented the Lava Regression algorithm on [GitHub at vldrenth/lavapy](https://github.com/vldrenth/lavapy). To see just how simple it is, I started by implementing it with `cvxpy`. However, this uses general convex solvers, and we can do a lot better! It turns out, lava can be solved by just using lasso in a clever way.

### Summary

If you suspect your data contains a few needles in a haystack (sparse) *plus* a lot of hay that actually matters (dense), the Elastic Net might be over-sparsifying your model. Lava offers a mathematically sound way to model the superposition of these two signal types directly.
